{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5bbd3f77",
   "metadata": {},
   "source": [
    "Copyright (c) Microsoft Corporation. All rights reserved.  \n",
    "Licensed under the MIT License.\n",
    "\n",
    "## Inference Phi-2 Model with Olive and ONNX Runtime for High Performance and Cross Platform\n",
    "\n",
    "Microsoft Phi-2 is a 2.7 billion-parameter language model with reasoning and language understanding capabilities. \n",
    "\n",
    "This tutorial includes instructions on optimizing the Phi-2 model from HF with Olive for different hardware targets, as well as runing the optimized model using the ONNX Runtime Generative API for high performance across platforms. \n",
    "\n",
    "### Steps\n",
    "\n",
    "0. **Prerequisites** - install packages and get model access\n",
    "1. **Optimize Phi-2 for hardware target** - run with Olive for generating hardware-specific models\n",
    "2. **Run Phi-2 with high performance** - run with ONNX Runtime Generative() API, and compare the performance with llama.cpp\n",
    "3. **Run Phi-2 everywhere** - Phi-2 in Windows APP, Mobile APP and Web APP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2663cec1",
   "metadata": {},
   "source": [
    "### Step 0 - Prerequisites\n",
    "Install all required packages and obtain access to the model on Hugging Face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cdce572-02eb-421e-a1c2-d169a4c85e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip uninstall onnxruntime-genai\n",
    "#!pip uninstall onnxruntime\n",
    "!pip install onnxruntime-genai-cuda --pre --index-url=https://aiinfra.pkgs.visualstudio.com/PublicPackages/_packaging/onnxruntime-genai/pypi/simple/\n",
    "!pip install onnxruntime-gpu\n",
    "!pip install olive\n",
    "!pip install huggingface-hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51ea1db1-4460-49ed-8da5-918a1adb460d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!huggingface-cli login --token <TOKEN>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fde0ada",
   "metadata": {},
   "source": [
    "### Step 1 - Optimize Phi-2 for hardware target\n",
    "Olive is a hardware-aware model optimization tool that efficiently generates optimized models to run with the ONNX Runtime. \n",
    "\n",
    "By specifying the input model and the targeted hardware in a configuration file, Olive applies cutting-edge optimizations to obtain the optimized model with a single line of code. \n",
    "\n",
    "Here, we use olive for generating optimized models for both CPU and GPU. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e86b3453-0e9f-47fb-ab63-193e0e06c583",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-11 17:01:59,213 olive.workflows.run.run [INFO] - Loading Olive module configuration from: c:\\Users\\qining\\AppData\\Local\\anaconda3\\envs\\phi2\\Lib\\site-packages\\olive\\olive_config.json\n",
      "2024-04-11 17:01:59,215 olive.workflows.run.run [INFO] - Loading run configuration from: phi2_cpu.json\n",
      "2024-04-11 17:01:59,216 olive.workflows.run.config [INFO] - No evaluator is specified, skip to evaluate model\n",
      "2024-04-11 17:01:59,217 olive.hardware.accelerator [INFO] - Running workflow on accelerator specs: cpu-cpu\n",
      "2024-04-11 17:01:59,217 olive.workflows.run.run [INFO] - Importing pass module GenAIModelExporter\n",
      "2024-04-11 17:01:59,218 olive.engine.engine [INFO] - Using cache directory: cache\n",
      "2024-04-11 17:01:59,219 olive.engine.engine [INFO] - Running Olive on accelerator: cpu-cpu\n",
      "2024-04-11 17:01:59,220 olive.engine.engine [INFO] - Running pass genai_exporter:GenAIModelExporter\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading embedding layer\n",
      "Reading decoder layer 0\n",
      "Reading decoder layer 1\n",
      "Reading decoder layer 2\n",
      "Reading decoder layer 3\n",
      "Reading decoder layer 4\n",
      "Reading decoder layer 5\n",
      "Reading decoder layer 6\n",
      "Reading decoder layer 7\n",
      "Reading decoder layer 8\n",
      "Reading decoder layer 9\n",
      "Reading decoder layer 10\n",
      "Reading decoder layer 11\n",
      "Reading decoder layer 12\n",
      "Reading decoder layer 13\n",
      "Reading decoder layer 14\n",
      "Reading decoder layer 15\n",
      "Reading decoder layer 16\n",
      "Reading decoder layer 17\n",
      "Reading decoder layer 18\n",
      "Reading decoder layer 19\n",
      "Reading decoder layer 20\n",
      "Reading decoder layer 21\n",
      "Reading decoder layer 22\n",
      "Reading decoder layer 23\n",
      "Reading decoder layer 24\n",
      "Reading decoder layer 25\n",
      "Reading decoder layer 26\n",
      "Reading decoder layer 27\n",
      "Reading decoder layer 28\n",
      "Reading decoder layer 29\n",
      "Reading decoder layer 30\n",
      "Reading decoder layer 31\n",
      "Reading final norm\n",
      "Reading LM head\n",
      "Saving ONNX model in D:\\qining\\Repo\\assistant\\cache\\models\\1_GenAIModelExporter-1473a6e460df1ddcd4cf088ff0019b1e-fe48ab55cdf4d03b843ede7c3c3be27b-cpu-cpu\\output_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving GenAI config in D:\\qining\\Repo\\assistant\\cache\\models\\1_GenAIModelExporter-1473a6e460df1ddcd4cf088ff0019b1e-fe48ab55cdf4d03b843ede7c3c3be27b-cpu-cpu\\output_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-11 17:03:00,517 olive.engine.engine [INFO] - Pass genai_exporter:GenAIModelExporter finished in 61.295080 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving processing files in D:\\qining\\Repo\\assistant\\cache\\models\\1_GenAIModelExporter-1473a6e460df1ddcd4cf088ff0019b1e-fe48ab55cdf4d03b843ede7c3c3be27b-cpu-cpu\\output_model for GenAI\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-11 17:03:01,346 olive.engine.engine [INFO] - Save footprint to phi-2\\cpu\\int4\\cpu-cpu_footprints.json.\n",
      "2024-04-11 17:03:01,347 olive.engine.engine [INFO] - Run history for cpu-cpu:\n",
      "2024-04-11 17:03:01,348 olive.engine.engine [INFO] - Please install tabulate for better run history output\n",
      "2024-04-11 17:03:01,349 olive.engine.engine [INFO] - No packaging config provided, skip packaging artifacts\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{AcceleratorSpec(accelerator_type=<Device.CPU: 'cpu'>, execution_provider='CPUExecutionProvider', vender=None, version=None, memory=None, num_cores=None): <olive.engine.footprint.Footprint at 0x1f27f203ed0>}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import olive.workflows\n",
    "\n",
    "olive.workflows.run(\"phi2_gpu.json\")\n",
    "\n",
    "#for getting CPU model, pls uncomment this line below\n",
    "#olive.workflows.run(\"phi2_cpu.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24485a1d",
   "metadata": {},
   "source": [
    "### Step 2 - Run Phi-2 with high performance\n",
    "ONNX Runtime is a high-performance, cross-platform engine for running AI models. \n",
    "\n",
    "ONNX Runtime Generate() API is the tailored solution for optimizing generative AI models. \n",
    "\n",
    "After obtaining an optimized Phi-2 model, we can input it into the ONNX Runtime Generative API for high-performance inference."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69a3ce71",
   "metadata": {},
   "source": [
    "##### Step 2.1 Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "add45ace-14be-4ab3-a68c-303aebeea18c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model...\n",
      "Model loaded in 17.13 seconds\n"
     ]
    }
   ],
   "source": [
    "import onnxruntime_genai as og\n",
    "import time\n",
    "\n",
    "print(\"Loading model...\")\n",
    "app_started_timestamp = time.time()\n",
    "\n",
    "model = og.Model(f'.\\\\phi-2\\\\cuda\\\\int4\\\\genai_exporter\\\\gpu-cuda_model')\n",
    "#for running CPU model, pls uncomment this line below\n",
    "#model = og.Model(f'.\\\\phi-2\\\\cpu\\\\int4\\\\genai_exporter\\\\cpu-cpu_model')\n",
    "\n",
    "model_loaded_timestamp  = time.time()\n",
    "\n",
    "print(\"Model loaded in {:.2f} seconds\".format(model_loaded_timestamp - app_started_timestamp))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2528501b",
   "metadata": {},
   "source": [
    "##### Step 2.2 Load tokenizer, set prompt and question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79513969-40bc-4588-a10c-8c482d224fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading tokenizer...\")\n",
    "tokenizer = og.Tokenizer(model)\n",
    "tokenizer_stream = tokenizer.create_stream()\n",
    "\n",
    "print(\"Tokenizer created\")\n",
    "\n",
    "system_prompt = \"You are a helpful assistant. Answer in one sentence.\"\n",
    "text = \"What is Dilithium?\"\n",
    "\n",
    "input_tokens = tokenizer.encode(system_prompt + text)\n",
    "\n",
    "prompt_length = len(input_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d115a28",
   "metadata": {},
   "source": [
    "##### Step 2.3 Run Phi-2 model and measure performance with ONNX Runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dcf8cc3-d5d2-42b1-8ad1-76d6629667b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "started_timestamp = time.time()\n",
    "\n",
    "print(\"Creating generator ...\")\n",
    "params = og.GeneratorParams(model)\n",
    "params.set_search_options({\"do_sample\": False, \"max_length\": 2028, \"min_length\": 0, \"top_p\": 0.9, \"top_k\": 40, \"temperature\": 1.0, \"repetition_penalty\": 1.0})\n",
    "params.input_ids = input_tokens\n",
    "generator = og.Generator(model, params)\n",
    "print(\"Generator created\")\n",
    "\n",
    "first = True\n",
    "new_tokens = []\n",
    "\n",
    "while not generator.is_done():\n",
    "    generator.compute_logits()\n",
    "    generator.generate_next_token()\n",
    "    if first:\n",
    "        first_token_timestamp = time.time()\n",
    "        first = False\n",
    "\n",
    "    new_token = generator.get_next_tokens()[0]\n",
    "    print(tokenizer_stream.decode(new_token), end=\"\")\n",
    "    new_tokens.append(new_token)\n",
    "\n",
    "print()\n",
    "run_time = time.time() - started_timestamp\n",
    "print(f\"Prompt tokens: {len(input_tokens)}, New tokens: {len(new_tokens)}, Time to first: {(first_token_timestamp - started_timestamp):.2f}s, New tokens per second: {len(new_tokens)/run_time:.2f} tps\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b21ba84-3449-4639-9a2f-1ce38760f61c",
   "metadata": {},
   "source": [
    "##### Step 2.4 Compare with llama.cpp\n",
    "\n",
    "llama.cpp is another popular solution to enable LLM inference with high performance on a wide variety of hardware targets. It now supports a small set of models. \n",
    "\n",
    "For a PyTorch model, it also requires conversion and optimization to its model format, known as GGUF format (https://github.com/ggerganov/llama.cpp?tab=readme-ov-file#prepare-and-quantize). Also you can download pre-optimized GGUF models on Hugging face. \n",
    "\n",
    "You can skip these steps below if llama.cpp has been built already. \n",
    "\n",
    "**Download and build llama.cpp**\n",
    "* git clone https://github.com/ggerganov/llama.cpp.git\n",
    "* cd llama.cpp\n",
    "* cmake -S . -B build/ -D CMAKE_BUILD_TYPE=Release\n",
    "* cmake --build build/ --config Release\n",
    "\n",
    "**Download gguf phi-2 model**\n",
    "* git lfs install\n",
    "* git clone https://huggingface.co/TheBloke/phi-2-GGUF\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfd4e897-1316-4f80-8fe1-0088341be5b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare with llama.cpp.\n",
    "\n",
    "! ..\\..\\ggerganov\\llama.cpp\\build\\bin\\Release\\main -m ..\\..\\thebloke\\phi-2-GGUF\\phi-2.Q4_K_M.gguf --prompt \"You are a helpful assistant. Answer in one sentence. What is Dilithium?\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2324685f",
   "metadata": {},
   "source": [
    "### Step 3 - Run Phi-2 everywhere\n",
    "* Windows APP with Phi-2 - APP folder (to be added)\n",
    "* Web APP with Phi-2: (https://guschmue.github.io/ort-webgpu/chat/?model=phi2)\n",
    "* Moble APP with Phi-2: Cast Android phone screen to Laptop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0b2672d-2f25-4f89-9d74-d87c37f453d2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
